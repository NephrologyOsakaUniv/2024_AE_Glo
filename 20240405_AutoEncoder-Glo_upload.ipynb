{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import glob, shutil, os, random, pathlib, pprint, scipy, umap\n",
    "import seaborn as sns; sns.set()\n",
    "import random\n",
    "\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers, losses\n",
    "from tensorflow.keras.layers import Input, Dense, Conv2D, Flatten, Dropout, MaxPooling2D, Activation, UpSampling2D\n",
    "from tensorflow.keras.layers import  MaxPool2D, BatchNormalization, Reshape, Conv2DTranspose, LeakyReLU, Rescaling\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.optimizers import Adam, SGD\n",
    "from PIL import Image, ImageEnhance, ImageOps\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"] = 'PCI_BUS_ID'\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = '1'\n",
    "\n",
    "Image.MAX_IMAGE_PIXELS = 10000000000\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "strategy = tf.distribute.MirroredStrategy()\n",
    "print('Number of devices: {}'.format(strategy.num_replicas_in_sync))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "####################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Seeding ##\n",
    "np.random.seed(0)\n",
    "tf.random.set_seed(0)\n",
    "\n",
    "IMG_SIZE = 256\n",
    "BATCH_SIZE = 256\n",
    "WORKERS = 32\n",
    "Channel = 3\n",
    "latent_dim = 256"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dir = 'train_dir'\n",
    "val_dir = 'val_dir'\n",
    "\n",
    "train_ds = tf.keras.utils.image_dataset_from_directory(\n",
    "    train_dir,\n",
    "    labels=None,\n",
    "    label_mode=None,\n",
    "    class_names=None,\n",
    "    color_mode='rgb',\n",
    "    batch_size=BATCH_SIZE,\n",
    "    image_size=(IMG_SIZE, IMG_SIZE),\n",
    "    shuffle=True,\n",
    "    seed=123,\n",
    "    validation_split=None,\n",
    "    subset=None,\n",
    "    interpolation='bilinear',\n",
    "    follow_links=False,\n",
    "    crop_to_aspect_ratio=False)\n",
    "\n",
    "val_ds = tf.keras.utils.image_dataset_from_directory(\n",
    "    val_dir,\n",
    "    labels=None,\n",
    "    label_mode=None,\n",
    "    class_names=None,\n",
    "    color_mode='rgb',\n",
    "    batch_size=BATCH_SIZE,\n",
    "    image_size=(IMG_SIZE, IMG_SIZE),\n",
    "    shuffle=True,\n",
    "    seed=123,\n",
    "    validation_split=None,\n",
    "    subset=None,\n",
    "    interpolation='bilinear',\n",
    "    follow_links=False,\n",
    "    crop_to_aspect_ratio=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 10))\n",
    "for images in train_ds.take(1):\n",
    "    for i in range(9):\n",
    "        ax = plt.subplot(3, 3, i + 1)\n",
    "        plt.imshow(images[i].numpy().astype(\"uint8\"))\n",
    "        #plt.title(class_names[labels[i]])\n",
    "        plt.axis(\"off\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###########################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "AUTOTUNE = tf.data.AUTOTUNE\n",
    "\n",
    "train_ds = train_ds.cache().prefetch(buffer_size=AUTOTUNE)\n",
    "val_ds = val_ds.cache().prefetch(buffer_size=AUTOTUNE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "normalization_layer = layers.Rescaling(1./255)\n",
    "normalized_train_ds = train_ds.map(lambda x: (normalization_layer(x), normalization_layer(x)))\n",
    "normalized_val_ds = val_ds.map(lambda x: (normalization_layer(x), normalization_layer(x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = Input(shape=(IMG_SIZE, IMG_SIZE, Channel), name=\"inputs\")\n",
    "x = inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = Conv2D(32, (3, 3), padding=\"same\")(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = LeakyReLU(alpha=0.2)(x)\n",
    "x = MaxPool2D((2, 2))(x)\n",
    "\n",
    "x = Conv2D(16, (3, 3), padding=\"same\")(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = LeakyReLU(alpha=0.2)(x)\n",
    "x = MaxPool2D((2, 2))(x)\n",
    "\n",
    "x = Conv2D(8, (3, 3), padding=\"same\")(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = LeakyReLU(alpha=0.2)(x)\n",
    "x = MaxPool2D((2, 2))(x)\n",
    "\n",
    "x = Conv2D(8, (3, 3), padding=\"same\")(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = LeakyReLU(alpha=0.2)(x)\n",
    "temp = MaxPool2D((2, 2))(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = Flatten()(temp)\n",
    "units = x.shape[1]\n",
    "encoded = Dense(latent_dim, name=\"latent\")(x)\n",
    "x = Dense(units)(encoded)\n",
    "x = LeakyReLU(alpha=0.2)(x)\n",
    "x = Reshape((temp.shape[1], temp.shape[2], temp.shape[3]))(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = Conv2DTranspose(8, (3, 3), strides=2, padding=\"same\")(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = LeakyReLU(alpha=0.2)(x)\n",
    "\n",
    "x = Conv2DTranspose(8, (3, 3), strides=2, padding=\"same\")(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = LeakyReLU(alpha=0.2)(x)\n",
    "\n",
    "x = Conv2DTranspose(16, (3, 3), strides=2, padding=\"same\")(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = LeakyReLU(alpha=0.2)(x)\n",
    "\n",
    "x = Conv2DTranspose(32, (3, 3), strides=2, padding=\"same\")(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = LeakyReLU(alpha=0.2)(x)\n",
    "\n",
    "x = Conv2DTranspose(Channel, (3, 3), strides=1, padding=\"same\")(x)\n",
    "#x = BatchNormalization()(x)\n",
    "x = Activation(\"sigmoid\", name=\"outputs\")(x)\n",
    "\n",
    "outputs = x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "autoencoder = Model(inputs, outputs)\n",
    "autoencoder.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = Model(inputs, encoded)\n",
    "encoder.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#autoencoder.load_weights('weight.h5')\n",
    "#autoencoder.weights\n",
    "#encoder.weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train AE #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "autoencoder.compile(tf.keras.optimizers.Adam(learning_rate=1e-3), loss=losses.MeanSquaredError())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS = 100\n",
    "\n",
    "CSV_name = 'save.csv'\n",
    "filepath = 'save.ep{epoch:02d}_vl{val_loss:.2f}.h5'\n",
    "\n",
    "CALLBACKS = [tf.keras.callbacks.ModelCheckpoint(filepath=filepath,\n",
    "                                                monitor=\"val_loss\",\n",
    "                                                verbose=0,\n",
    "                                                save_best_only=False,\n",
    "                                                save_weights_only=True,\n",
    "                                                mode=\"auto\",\n",
    "                                                save_freq=\"epoch\"),\n",
    "             tf.keras.callbacks.EarlyStopping(monitor=\"val_loss\",\n",
    "                                              min_delta=0,\n",
    "                                              patience=5,\n",
    "                                              verbose=0,\n",
    "                                              mode=\"auto\",\n",
    "                                              baseline=None,\n",
    "                                              restore_best_weights=True),\n",
    "             tf.keras.callbacks.ReduceLROnPlateau(monitor=\"val_loss\",\n",
    "                                                  factor=0.3,\n",
    "                                                  patience=3,\n",
    "                                                  verbose=0,\n",
    "                                                  mode=\"auto\",\n",
    "                                                  min_delta=0.0000001,\n",
    "                                                  cooldown=0,\n",
    "                                                  min_lr=0),\n",
    "             tf.keras.callbacks.CSVLogger(CSV_name, separator=\",\", append=False)]\n",
    "\n",
    "\n",
    "hist = autoencoder.fit(normalized_train_ds, \n",
    "    batch_size=BATCH_SIZE,\n",
    "    epochs=EPOCHS,\n",
    "    verbose='auto',\n",
    "    callbacks=CALLBACKS,\n",
    "    validation_split=0.0,\n",
    "    validation_data=normalized_val_ds,\n",
    "    shuffle=True,\n",
    "    class_weight=None,\n",
    "    sample_weight=None,\n",
    "    initial_epoch=0,\n",
    "    steps_per_epoch=None,\n",
    "    validation_steps=None,\n",
    "    validation_batch_size=BATCH_SIZE,\n",
    "    validation_freq=1,\n",
    "    max_queue_size=10,\n",
    "    workers=32,\n",
    "    use_multiprocessing=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# encode #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dir = 'test_data_dir'\n",
    "test_datagen = tf.keras.preprocessing.image.ImageDataGenerator(featurewise_center=False,\n",
    "                                                              samplewise_center=False,\n",
    "                                                              featurewise_std_normalization=False,\n",
    "                                                              samplewise_std_normalization=False,\n",
    "                                                              zca_whitening=False,\n",
    "                                                              zca_epsilon=1e-6,\n",
    "                                                              rotation_range=0,\n",
    "                                                              width_shift_range=0,\n",
    "                                                              height_shift_range=0,\n",
    "                                                              brightness_range=None,\n",
    "                                                              shear_range=0.0,\n",
    "                                                              zoom_range=0.0,\n",
    "                                                              channel_shift_range=0.0,\n",
    "                                                              fill_mode='nearest',\n",
    "                                                              cval=0.0,\n",
    "                                                              horizontal_flip=False,\n",
    "                                                              vertical_flip=False,\n",
    "                                                              rescale=1./255,\n",
    "                                                              preprocessing_function=None,\n",
    "                                                              data_format=None,\n",
    "                                                              validation_split=0.0,\n",
    "                                                              dtype=None)\n",
    "test_generator = test_datagen.flow_from_directory(test_dir, target_size=(IMG_SIZE, IMG_SIZE),\n",
    "                                                 color_mode='rgb', shuffle=False, batch_size=BATCH_SIZE,\n",
    "                                                 class_mode=None, interpolation='lanczos')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image = next(iter(test_generator))\n",
    "image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_filenames = test_generator.filepaths\n",
    "test_filenames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EncodedImages = encoder.predict(test_generator, workers=16, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save('FileName.npy', test_filenames)\n",
    "np.save('AE_EncodedImgs.npy', EncodedImages)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
